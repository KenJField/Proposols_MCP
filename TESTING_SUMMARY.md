# Testing Summary

## Overview

A comprehensive test suite has been created to validate the Proposal MCP Server and bring it to a working prototype state.

## Test Suite Components

### 1. Unit Tests (`tests/unit/`)

**Purpose:** Test individual components in isolation with mocked dependencies.

**Files:**
- `test_config.py` - Configuration validation
- `test_keywords.py` - Keyword extraction utilities
- `test_embeddings.py` - Embedding generation service
- `test_email.py` - Email validation service

**Run:**
```bash
pytest tests/unit/ -v
```

**Coverage:** Core utilities and services without external dependencies.

### 2. Integration Tests (`tests/integration/`)

**Purpose:** Test database operations and tool integrations.

**Files:**
- `test_database.py` - Database schema and connection tests
- `test_tools.py` - MCP tool integration tests

**Run:**
```bash
pytest tests/integration/ -v -m integration
```

**Requirements:** Supabase credentials must be configured.

### 3. Test Scripts (`scripts/`)

**Purpose:** Utility scripts for testing and validation.

**Files:**
- `quick_test.py` - Quick validation of basic setup
- `validate_deployment.py` - Comprehensive deployment validation
- `load_test_data.py` - Load test data for manual testing
- `test_mcp_tools.py` - Test MCP tools directly

**Usage:**
```bash
# Quick validation
python scripts/quick_test.py

# Full deployment check
python scripts/validate_deployment.py

# Load test data
python scripts/load_test_data.py

# Test tools
python scripts/test_mcp_tools.py
```

### 4. Test Data

**Location:** Generated by `scripts/load_test_data.py`

**Contents:**
- 3 Internal Resources (staff, assets)
- 2 External Resources (vendors)
- 3 Policies (pricing, legal, technical)
- 1 Sample RFP
- 3 Experience Entries

**Features:**
- Realistic data for testing
- Proper tenant isolation
- Ready for embedding generation

### 5. Test Documentation

**Files:**
- `TEST_PLAN.md` - Comprehensive manual testing checklist
- `README_TESTING.md` - Testing guide and quick reference
- `PROTOTYPE_CHECKLIST.md` - Pre-launch validation checklist

## Quick Start Testing

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Configure Environment

```bash
export SUPABASE_URL="https://your-project.supabase.co"
export SUPABASE_SERVICE_ROLE_KEY="your-key"
export OPENAI_API_KEY="your-key"
```

### Step 3: Run Quick Test

```bash
python scripts/quick_test.py
```

### Step 4: Run Unit Tests

```bash
pytest tests/unit/ -v
```

### Step 5: Load Test Data

```bash
python scripts/load_test_data.py
```

### Step 6: Validate Deployment

```bash
python scripts/validate_deployment.py
```

### Step 7: Test MCP Tools

```bash
python scripts/test_mcp_tools.py
```

## Test Coverage

### Unit Tests
- ✅ Configuration validation
- ✅ Keyword extraction
- ✅ Embedding service (mocked)
- ✅ Email service (mocked)

### Integration Tests
- ✅ Database connection
- ✅ Table existence
- ✅ Function existence
- ✅ Tool integration (mocked)

### Manual Tests (see TEST_PLAN.md)
- Database schema validation
- Hybrid search functions
- Embedding generation
- Edge Functions
- MCP server tools
- Validation workflow
- Multi-tenant isolation
- Performance
- Security

## Test Execution Order

### For New Deployment

1. **Quick Validation** (`scripts/quick_test.py`)
   - Verify configuration
   - Check database connection
   - Validate tables exist

2. **Unit Tests** (`pytest tests/unit/`)
   - Ensure core components work
   - No external dependencies needed

3. **Load Test Data** (`scripts/load_test_data.py`)
   - Create sample data
   - Wait for embeddings (1-2 minutes)

4. **Integration Tests** (`pytest tests/integration/`)
   - Test with real database
   - Verify functions work

5. **Deployment Validation** (`scripts/validate_deployment.py`)
   - Comprehensive system check
   - Verify all components

6. **Tool Testing** (`scripts/test_mcp_tools.py`)
   - Test MCP tools directly
   - Verify search and experience tools

7. **Manual Testing** (see TEST_PLAN.md)
   - End-to-end workflows
   - User scenarios
   - Performance validation

## Success Criteria

### Must Pass (Critical)
- [x] All unit tests pass
- [x] Database schema deployed
- [x] Test data loads successfully
- [x] Deployment validation passes
- [ ] MCP server starts without errors
- [ ] Tools are callable via MCP client

### Should Pass (Important)
- [ ] Integration tests pass
- [ ] Embeddings generate automatically
- [ ] Search functions return results
- [ ] Validation workflow works end-to-end

### Nice to Have (Optional)
- [ ] Performance benchmarks
- [ ] Load testing
- [ ] Security audit

## Troubleshooting

### Tests Fail

1. **Check Configuration**
   ```bash
   python -c "from src.config import Config; Config.validate()"
   ```

2. **Verify Database**
   ```bash
   python scripts/quick_test.py
   ```

3. **Check Test Data**
   ```sql
   SELECT COUNT(*) FROM internal_resources;
   SELECT COUNT(*) FROM embedding_queue WHERE status = 'pending';
   ```

### Embeddings Not Generating

1. Check Edge Function is deployed
2. Verify OpenAI API key in Edge Function secrets
3. Check embedding_queue table
4. Review Edge Function logs

### Search Returns No Results

1. Verify embeddings exist: `SELECT COUNT(*) FROM internal_resources WHERE embedding IS NOT NULL;`
2. Check search_vector is populated
3. Lower match_threshold parameter
4. Verify test data exists

## Next Steps

1. **Run All Tests**
   ```bash
   pytest -v
   python scripts/validate_deployment.py
   ```

2. **Load Test Data**
   ```bash
   python scripts/load_test_data.py
   ```

3. **Follow Prototype Checklist**
   - See `PROTOTYPE_CHECKLIST.md`
   - Complete all critical items
   - Document known issues

4. **Manual Testing**
   - Follow `TEST_PLAN.md`
   - Test all user workflows
   - Validate end-to-end scenarios

5. **Launch Prototype**
   - All critical tests pass
   - Known issues documented
   - Team notified

## Test Maintenance

### Adding New Tests

1. **Unit Tests:** Add to `tests/unit/`
2. **Integration Tests:** Add to `tests/integration/`
3. **Update Fixtures:** Modify `tests/conftest.py` if needed

### Updating Test Data

1. Modify `scripts/load_test_data.py`
2. Add new sample data
3. Update test fixtures in `conftest.py`

### Continuous Testing

For CI/CD pipelines:

```bash
# Fast unit tests only
pytest tests/unit/ -v

# With coverage
pytest --cov=src --cov-report=xml

# Integration tests (requires credentials)
pytest tests/integration/ -v -m integration
```

## Support

For issues or questions:
1. Check `TEST_PLAN.md` for detailed test procedures
2. Review `README_TESTING.md` for testing guide
3. Check `PROTOTYPE_CHECKLIST.md` for launch requirements
